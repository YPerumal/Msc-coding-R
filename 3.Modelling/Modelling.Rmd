# Supervised Learning

## Import Libraries
```{r message=FALSE, warning=FALSE}
library(xgboost)
library(tidyverse)
library(caret)
library(randomForest)
library(ggplot2)
library(rpart)
```

## Import Data
```{r}
# df_wide <- read.csv('/Users/yevashanperumal/Library/CloudStorage/OneDrive-UniversityofCapeTown/Yevashan Perumal MSc - OneDrive/Data/wide_data.csv')

# df_long <- read.csv('/Users/yevashanperumal/Library/CloudStorage/OneDrive-UniversityofCapeTown/Yevashan Perumal MSc - OneDrive/Data/long_data.csv')
```

```{r}
# load("/Users/yevashanperumal/Library/CloudStorage/OneDrive-UniversityofCapeTown/Yevashan Perumal MSc - OneDrive/Data/data_for_modelling.Rdata")
```


```{r}
load("/Users/yevashanperumal/Library/CloudStorage/OneDrive-UniversityofCapeTown/Yevashan Perumal MSc - OneDrive/Data/modelling_image.Rdata")
```



```{r}
#finding zero variance columns
# zv <- apply(df_long %>% select_if(is.numeric), 2, function(x) length(unique(x)) == 1)
# sum(zv)
# zv

# Find names of highly correlated columns
# df_corr = cor(df_long %>% select_if(is.numeric))
# hc = findCorrelation(df_corr, cutoff=0.6) # putt any value as a "cutoff"
# hc = sort(hc)
# #the name of the columns chosen above
# to_be_removed <- colnames(df_corr)[hc]

# Remove highly correlated columns
#Not sure if all should be removed or just some.
df_long <- df_long %>% select(-CurrentCoverAmount,-WeightValue,-SystolicBloodPressureValue,-NumberOfDeferrals,-ClientBMIValue)
```



## Train-Test Split for df_long
```{r}
# Create the training and test datasets
set.seed(42)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(df_long$GammaGTValue, p=0.8, list=FALSE)

# Step 2: Create the training  dataset
train_df <- df_long[trainRowNumbers,]
x_train <- train_df %>% select(-GammaGTValue)
y_train <- train_df %>% select(GammaGTValue)

# Step 3: Create the test dataset
test_df <- df_long[-trainRowNumbers,]
x_test <- test_df %>% select(-GammaGTValue)
y_test <- test_df %>% select(GammaGTValue)
```

## Decision Tree
```{r}
# Random forest
# Set train control for cross-val
dt_ctrl <- trainControl(method = 'cv', number = 5, verboseIter = F)

# #rpart
# dt_grid <- expand.grid(cp = c(5,1)
#                        ) #Default for regression is 5. Controls tree size.
# # rpart2
# dt_grid <- expand.grid(maxdepth = c(5,10,20)
#                        ) #Default for regression is 5. Controls tree size.

set.seed(42)
dt_gridsearch <- train(GammaGTValue ~ ., 
                       data = train_df,
                       method = 'rpart',
                       trControl = dt_ctrl
                       # ,tuneGrid = dt_grid #Here is the grid
                       ) 

# dt_gridsearch$bestTune
cp <- dt_gridsearch$bestTune[[1]]

# train a final model to get variable importance plot
library(rpart)
dt_final <-  rpart(GammaGTValue ~ ., data = train_df,cp=cp)
#Predictions
dt_grid_pred <- predict(dt_final, newdata = test_df)
dt_rmse <-  sqrt(mean((test_df$GammaGTValue- dt_grid_pred)^2))
```

## Random Forest Using Caret
```{r}
# Random forest
# Set train control for cross-val
rf_ctrl <- trainControl(method = 'cv', number = 5, verboseIter = F)

# Set grid search for hyperparaneter search
rf_grid <- expand.grid(mtry = 3:5,
                       splitrule = 'variance', #Have to specify. This is RSS for reg.
                       min.node.size = c(1,5)) #Default for regression is 5. Controls tree size.

# set.seed(42)
rf_gridsearch <- train(GammaGTValue ~ ., 
                       data = train_df,
                       method = 'ranger',
                       num.trees = 10,
                       verbose = T,
                       trControl = rf_ctrl
                       ,tuneGrid = rf_grid #Here is the grid
                       ) 

rf_gridsearch$bestTune
mtry <- rf_gridsearch$bestTune[[1]]
splitrule <- rf_gridsearch$bestTune[[2]]
min.node.size <- rf_gridsearch$bestTune[[3]]

# train a final model to get variable importance plot
rf_final <- randomForest(GammaGTValue ~ ., data = train_df,
                           ntree = 10,
                           importance = TRUE,
                            mtry=mtry,
                            splitrule=splitrule,
                            min.node.size=min.node.size)

#Predictions
rf_grid_pred <- predict(rf_final, newdata = test_df)
rf_rmse <-  sqrt(mean((test_df$GammaGTValue- rf_grid_pred)^2))
```

## XGB using Caret
```{r}
# XGboost model
xgb_grid <- expand.grid(nrounds = c(10,25),  #B - number of trees
                        max_depth = c(1,5),      #d - interaction depth
                        eta = c(0.1,0.01),       #lambda - learning rate
                        gamma = 0.001,            #mindev
                        colsample_bytree = 1,     #proportion random features per tree
                        min_child_weight = 1,     #also controls tree depth
                        subsample = 1             #bootstrap proportion
)
ctrl <-  trainControl(method = 'cv', number = 5, verboseIter = T)

xgb_final <- train(GammaGTValue ~ ., data = train_df,
                 method = 'xgbTree',
                 trControl = ctrl,
                 verbose = F,
                 tuneGrid = xgb_grid)

#Predictions
xgb_pred <- predict(xgb_final, test_df)
xgb_rmse <- sqrt(mean((test_df$GammaGTValue - xgb_pred)^2))

```

#######################################################################

# Imputation 

## libraries
```{r}
library(mice)
```

## Set up data
```{r}
# copy train data 
imput_train_df <- train_df

#copy tet data
imput_test_df <- test_df
# make the test data target column nulls
imput_test_df$GammaGTValue <- NA

#combine into one
imput_data <- imput_train_df %>% bind_rows(imput_test_df)
```

## MICE
```{r}
system.time(
imp <- mice(imput_data,seed = 42) 
)
# imp <- mice(head(imput_data,31500),seed = 42) 
imp <- mice(imput_data,seed = 42) 
print(imp)

imp$imp$GammaGTValue
# meth = init$method
# predM = init$predictorMatrix
```
Error here. looks to be with highly correlted variables due to the factor vars I have. Need to either decide to remove, or use different methods for those ones.
looks like it starts acting up at more than 31500 rows. Need to investigate further what's gonig on here.        
 Ë†
```{r}
save.image("/Users/yevashanperumal/Library/CloudStorage/OneDrive-UniversityofCapeTown/Yevashan Perumal MSc - OneDrive/Data/modelling_image.Rdata")
```



