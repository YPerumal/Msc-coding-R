To Do:
- Set identifier columns in the imputataion; specific argument or do I need to do the index thing again? Could maybe just not reset index and the end of the last PreProcessing step

- pick on imputation method to start. get it to work successfully. Looks like the CART in MICE might work, double check what it means

- sort out the train test splits. Wide data has the least rows so needs to come from here. Do a counts to see what and 80/20 split gives me for all, pick a number of rows right now the middle. Get rows and indexes from wide set, filter other two datasets on the same indexes. Now I have my train test splits.

- can repeat the process and create other train test split datasets.


# Import Libraries
```{r message=FALSE, warning=FALSE}
library(xgboost)
library(tidyverse)
library(caret)
library(randomForest)
library(ggplot2)
library(rpart)
library(tree)

#Imputation
library(mice)
library(Amelia)
library(Rcpp)
library('fastDummies')
library(missForest)
```

# Import Data

```{r}
# Mac
# load("/Users/yevashanperumal/Library/CloudStorage/OneDrive-UniversityofCapeTown/Yevashan Perumal MSc - OneDrive/Data/data_for_modelling.Rdata")

# Windows load tables saved as Rdata to preserve types
load("C:/Users/X464371/OneDrive - University of Cape Town/Yevashan Perumal MSc - OneDrive/Data/data_for_modelling.Rdata")

# load images
# load("/Users/yevashanperumal/Library/CloudStorage/OneDrive-UniversityofCapeTown/Yevashan Perumal MSc - OneDrive/Data/imput_splits_image.Rdata")
```

# Some more cleaning
```{r}
#finding zero variance columns; was an issue during imputation

# zv <- apply(df_long %>% select_if(is.numeric), 2, function(x) length(unique(x)) == 1)
# sum(zv)
# zv

# Find names of highly correlated columns
# df_corr = cor(df_long %>% select_if(is.numeric))
# hc = findCorrelation(df_corr, cutoff=0.6) # putt any value as a "cutoff"
# hc = sort(hc)
# #the name of the columns chosen above
# to_be_removed <- colnames(df_corr)[hc]
```


```{r}
# Remove highly correlated columns
# Not sure if all should be removed or just some.

df_long <- df_long %>% select(-CurrentCoverAmount,-WeightValue,-SystolicBloodPressureValue,-NumberOfDeferrals,-ClientBMIValue)

df_wide <- df_wide %>% select(-CurrentCoverAmount,-WeightValue,-SystolicBloodPressureValue,-NumberOfDeferrals,-ClientBMIValue) # might need to do the same exercise as above when I get to the imputation

to_be_imputed <- to_be_imputed %>% select(-CurrentCoverAmount,-WeightValue,-SystolicBloodPressureValue,-NumberOfDeferrals,-ClientBMIValue)
```

# Imputation 

## MICE
```{r}
# system.time(
# imp <- mice(imput_data,seed = 42) 
# )
imp <- mice(to_be_imputed,method='cart',seed = 42,m=2,maxit=2)
# imp <- mice(head(to_be_imputed,1000),seed = 42) 


sapply(imp$data, function(x) sum(is.na(x)))
# this works
# test <- imput_data %>% select_if(is.numeric)
# imp <- mice(test,seed = 42)

# imp$imp$GammaGTValue
# meth = init$method
# predM = init$predictorMatrix
```
Error here. looks to be with highly correlted variables due to the factor vars I have. Need to either decide to remove, or use different methods for those ones.
looks like it starts acting up at more than 31500 rows. Need to investigate further what's gonig on here.
 
## Amelia
assumes varaibles follow multivariate normal distributions. Some transfaormations may be needed?
```{r}
imput_data_dummy_var <- dummy_cols(imput_data, select_columns = c('ProductSoldType', 'SmokerStatus', 'OccupationClassCode', 'DisabilityIncomeClass', 'AutoDeferredIndicator', 'NTUIndicator', 'LOAIndicator', 'ExclusionIndicator', 'AcceleratorIndicator', 'EducationID_Override', 'SumAssuredBand', 'ASS_ETHNIC_GROUP', 'PROVINCE', 'FARMER_IND', 'PROF_MARKET_IND', 'PUBLIC_SECTOR_IND', 'PUBLIC_SECTOR_TYPE', 'MARITAL_STATUS', 'GENDER'),
           remove_selected_columns = TRUE)
```
 
```{r}
set.seed(42)
amelia(imput_data_dummy_var %>% select(-'ProductSoldType_Standalone With Accelerator', -SmokerStatus_SMOKER, -OccupationClassCode_E, -DisabilityIncomeClass_3, -AutoDeferredIndicator_Y, -NTUIndicator_Y, -LOAIndicator_Y, -ExclusionIndicator_Y,- AcceleratorIndicator_Y, -EducationID_Override_4, -`SumAssuredBand_M: R2,000,000.00 - R2,500,000.00`, -ASS_ETHNIC_GROUP_WHITE, -`PROVINCE_W CAPE`,- FARMER_IND_Y, -PROF_MARKET_IND_UNKNOWN, -PROF_MARKET_IND_Y, -PUBLIC_SECTOR_IND_UNKNOWN, -PUBLIC_SECTOR_IND_Y, -`PUBLIC_SECTOR_TYPE_WESTERN-CAPE PROVINCIAL GOVERMENT`, -MARITAL_STATUS_WIDOWED, -GENDER_MALE,-GENDER_UNKNOWN),m=5)


amelia(to_be_imputed,m=1)
```
 
## Random MissForest
```{r}
miss_forest <- missForest(to_be_imputed %>% select(unique_id),maxiter=2,ntree=5)
```

```{r}
sum(is.na(miss_forest$ximp$GammaGTValue))
```

# Train-Test Split
```{r}
# Create the training and test datasets
set.seed(42)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(df_long$GammaGTValue, p=0.8, list=FALSE)

# Step 2: Create the training  dataset
train_df <- df_long[trainRowNumbers,]
x_train <- train_df %>% select(-GammaGTValue)
y_train <- train_df %>% select(GammaGTValue)

# Step 3: Create the test dataset
test_df <- df_long[-trainRowNumbers,]
x_test <- test_df %>% select(-GammaGTValue)
y_test <- test_df %>% select(GammaGTValue)
```

```{r}
save.image("/Users/yevashanperumal/Library/CloudStorage/OneDrive-UniversityofCapeTown/Yevashan Perumal MSc - OneDrive/Data/imput_splits_image.Rdata")
```