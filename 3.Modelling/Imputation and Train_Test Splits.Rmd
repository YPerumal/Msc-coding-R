Done:
- Set identifier columns in the imputataion; specific argument or do I need to do the index thing again? Could maybe just not reset index and the end of the last PreProcessing step

- pick on imputation method to start. get it to work successfully. Looks like the CART in MICE might work, double check what it means

To Do:

- combine the imputed MICE datasets into 1 final one. 

- sort out the train test splits. Wide data has the least rows so needs to come from here. Do a counts to see what and 80/20 split gives me for all, pick a number of rows right now the middle. Get rows and indexes from wide set, filter other two datasets on the same indexes. Now I have my train test splits.

- can repeat the process and create other train test split datasets.


remake the wide and long from the imputed set by using indices.Actually will only be usefull for the long dataset

decided to
wide: complete case
long: imputing data with a few missing cases
all: missing data where some columns have a lot of missingness

long and all will have same number of rows

8000 rows test

find how many rows I have of data without a complete gamma gt value; message stefan

at some point in the methodology, when I write about the models I use - motivate why I picked the models I did. We are explicity using heuristics that are agnostic to the underlying physiological data. (i.e not using models from a medical context; you could but it's a lot more work.s o now we're using imputation + SL because it's a lot more data agnostic; does all the work; dont need medical expertise)

Research objective:
do the different types of data processing methods yielded by imputatuion affect model accuracy differently for the different models?


# Import Libraries
```{r message=FALSE, warning=FALSE}
library(xgboost)
library(tidyverse)
library(caret)
library(randomForest)
library(ggplot2)
library(rpart)
library(tree)

#Imputation
library(mice)
library(Amelia)
library(Rcpp)
library('fastDummies')
library(missForest)
```

# Import Data

```{r}
# Mac
# load("/Users/yevashanperumal/Library/CloudStorage/OneDrive-UniversityofCapeTown/Yevashan Perumal MSc - OneDrive/Data/data_for_modelling.Rdata")

# Windows load tables saved as Rdata to preserve types
load("C:/Users/X464371/OneDrive - University of Cape Town/Yevashan Perumal MSc - OneDrive/Data/data_for_modelling.Rdata")

# load images
# load("/Users/yevashanperumal/Library/CloudStorage/OneDrive-UniversityofCapeTown/Yevashan Perumal MSc - OneDrive/Data/imput_splits_image.Rdata")
```

# Some more cleaning
```{r}
#finding zero variance columns; was an issue during imputation

# zv <- apply(df_long %>% select_if(is.numeric), 2, function(x) length(unique(x)) == 1)
# sum(zv)
# zv

# Find names of highly correlated columns
# df_corr = cor(df_long %>% select_if(is.numeric))
# hc = findCorrelation(df_corr, cutoff=0.6) # putt any value as a "cutoff"
# hc = sort(hc)
# #the name of the columns chosen above
# to_be_removed <- colnames(df_corr)[hc]
```


```{r}
# Remove highly correlated columns
# Not sure if all should be removed or just some.

df_long <- df_long %>% select(-CurrentCoverAmount,-WeightValue,-SystolicBloodPressureValue,-NumberOfDeferrals,-ClientBMIValue)

df_wide <- df_wide %>% select(-CurrentCoverAmount,-WeightValue,-SystolicBloodPressureValue,-NumberOfDeferrals,-ClientBMIValue) # might need to do the same exercise as above when I get to the imputation

to_be_imputed <- to_be_imputed %>% select(-CurrentCoverAmount,-WeightValue,-SystolicBloodPressureValue,-NumberOfDeferrals,-ClientBMIValue)


dim(df_long)
dim(df_wide)
dim(to_be_imputed)
```

```{r}
# Visualise missingness
md.pattern(to_be_imputed)
```



# Imputation 

## MICE
```{r}
# system.time(
# imp <- mice(imput_data,seed = 42) 
# )
imp <- mice(to_be_imputed,method='cart',seed = 42,m=5,maxit=5)
# imp <- mice(head(to_be_imputed,1000),seed = 42) 


sapply(imp$data, function(x) sum(is.na(x)))
# this works
# test <- imput_data %>% select_if(is.numeric)
# imp <- mice(test,seed = 42)

# imp$imp$GammaGTValue
# meth = init$method
# predM = init$predictorMatrix
```
Error here. looks to be with highly correlted variables due to the factor vars I have. Need to either decide to remove, or use different methods for those ones.
looks like it starts acting up at more than 31500 rows. Need to investigate further what's gonig on here.
 
 
```{r}
# first imputed dataset
complete(imp,1)
```
```{r}
# second imputed dataset
complete(imp,2)
```
 
```{r}
plot(imp)
```
 
```{r}
# diagnostic checking to see if imputed values match
stripplot(imp, chl~.imp, pch=20, cex=2)
```
 

```{r}
summary(to_be_imputed)
```
 
```{r}
summary(complete(imp))
```
compare means in the two summaries
 
 
```{r}
pool(imp)
```
 
 
## Amelia
assumes varaibles follow multivariate normal distributions. Some transfaormations may be needed?
```{r}
imput_data_dummy_var <- dummy_cols(to_be_imputed, select_columns = c('ProductSoldType', 'SmokerStatus', 'OccupationClassCode', 'DisabilityIncomeClass', 'AutoDeferredIndicator', 'NTUIndicator', 'LOAIndicator', 'ExclusionIndicator', 'AcceleratorIndicator', 'EducationID_Override', 'SumAssuredBand', 'ASS_ETHNIC_GROUP', 'PROVINCE', 'FARMER_IND', 'PROF_MARKET_IND', 'PUBLIC_SECTOR_IND', 'PUBLIC_SECTOR_TYPE', 'MARITAL_STATUS', 'GENDER'),
           remove_selected_columns = TRUE)
```
 
```{r}
set.seed(42)
amelia(imput_data_dummy_var %>% select(-'ProductSoldType_Standalone With Accelerator', -SmokerStatus_SMOKER, -OccupationClassCode_E, -DisabilityIncomeClass_3, -AutoDeferredIndicator_Y, -NTUIndicator_Y, -LOAIndicator_Y, -ExclusionIndicator_Y,- AcceleratorIndicator_Y, -EducationID_Override_4, -`SumAssuredBand_M: R2,000,000.00 - R2,500,000.00`, -ASS_ETHNIC_GROUP_WHITE, -`PROVINCE_W CAPE`,- FARMER_IND_Y, -PROF_MARKET_IND_UNKNOWN, -PROF_MARKET_IND_Y, -PUBLIC_SECTOR_IND_UNKNOWN, -PUBLIC_SECTOR_IND_Y, -`PUBLIC_SECTOR_TYPE_WESTERN-CAPE PROVINCIAL GOVERMENT`, -MARITAL_STATUS_WIDOWED, -GENDER_MALE,-GENDER_UNKNOWN),m=5)


amelia(to_be_imputed,m=1)
```
 
## Random MissForest
```{r}
miss_forest <- missForest(to_be_imputed,maxiter=5,ntree=5)
```

```{r}
miss_forest$ximp
```


```{r}
sum(is.na(miss_forest$ximp))
```

##########################################################################################################################

# Train-Test Split
```{r}
# Create the training and test datasets
set.seed(42)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(df_long$GammaGTValue, p=0.8, list=FALSE)

# Step 2: Create the training  dataset
train_df <- df_long[trainRowNumbers,]
x_train <- train_df %>% select(-GammaGTValue)
y_train <- train_df %>% select(GammaGTValue)

# Step 3: Create the test dataset
test_df <- df_long[-trainRowNumbers,]
x_test <- test_df %>% select(-GammaGTValue)
y_test <- test_df %>% select(GammaGTValue)
```

```{r}
save.image("/Users/yevashanperumal/Library/CloudStorage/OneDrive-UniversityofCapeTown/Yevashan Perumal MSc - OneDrive/Data/imput_splits_image.Rdata")
```