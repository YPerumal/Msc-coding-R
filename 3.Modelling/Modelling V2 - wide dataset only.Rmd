# Import Libraries
```{r message=FALSE, warning=FALSE}
library(xgboost)
library(tidyverse)
library(caret)
library(randomForest)
library(ggplot2)
library(rpart)
library(tree)
library(beepr)
#Imputation
library(mice)
library(Amelia)
library(Rcpp)
library('fastDummies')
library(missForest)
```

# Import datasets 
```{r}
# From the Impuation and Splits script
# load("C:/Users/X464371/OneDrive - University of Cape Town/Yevashan Perumal MSc - OneDrive/Data/imput_splits_image.Rdata")

# Current saved workspace
load("C:/Users/X464371/OneDrive - University of Cape Town/Yevashan Perumal MSc - OneDrive/Data/post_imputation_output_data.Rdata")
```

# Basic Baseline
```{r}
std.dev <- sqrt(var(wide_train_df$GammaGTValue))
```

# Random Forest Using Caret
```{r}
# Random forest

random_forest_train_func <- function(train_data){
    # Set train control for cross-val
    rf_ctrl <- trainControl(method = 'cv',
                            number = 5,
                            verboseIter = F)
    
    # Set grid search for hyperparaneter search
    rf_grid <- expand.grid(mtry = 10:20,
                           splitrule = 'variance', #Have to specify. This is RSS for reg.
                           min.node.size = c(5:10)) #Default for regression is 5. Controls tree size.
    
    # set.seed(42)
    rf_gridsearch <- train(GammaGTValue ~ ., 
                           data = train_data,
                           method = 'ranger',
                           num.trees = 100,
                           verbose = T,
                           trControl = rf_ctrl
                           ,tuneGrid = rf_grid #Here is the grid
                           ) 
    
    # rf_gridsearch$bestTune
    # mtry <- rf_gridsearch$bestTune[[1]]
    # splitrule <- rf_gridsearch$bestTune[[2]]
    # min.node.size <- rf_gridsearch$bestTune[[3]]
    
    # train a final model to get variable importance plot
    # rf_final <- randomForest(GammaGTValue ~ ., data = train_df,
    #                            ntree = 10,
    #                            importance = TRUE,
    #                             mtry=mtry,
    #                             splitrule=splitrule,
    #                             min.node.size=min.node.size)

    
    return(rf_gridsearch)
}
```


```{r}
test_func <- function(model,test_data){
    #Predictions
    y_pred <- predict(model, newdata = test_data)
    rmse <-  sqrt(mean((test_data$GammaGTValue- y_pred)^2))
    
    return(list(y_pred,rmse))
}
```

wide dataset
```{r}
rf_wide_model<- random_forest_train_func(wide_train_df %>% slice_sample(n=1000))
rf_wide_model_result <- test_func(rf_wide_model,mf_test_df)
```

miss forest dataset all
```{r}
rf_mf_all_model <- random_forest_train_func(mf_train_df  %>% slice_sample(n=1000))
rf_mf_all_model_result <- test_func(rf_mf_all_model,mf_test_df)
```

miss forest with fewer columns
```{r}
rf_mf_less_cols_model <- random_forest_train_func(mf_train_less_cols_df %>% slice_sample(n=1000))
rf_mf_less_cols_model_result <- test_func(rf_mf_less_cols_model,mf_test_df)
```

MICE imputed all columns agg dataset
```{r}
rf_mice_grouped_all_model <- random_forest_train_func(grouped_imputed_df%>% slice_sample(n=1000))
rf_mice_grouped_all_model_result <- test_func(rf_mice_grouped_all_model,mf_test_df)
```

MICE imputed fewer columns agg dataset
```{r}
rf_mice_grouped_less_cols_model <- random_forest_train_func(grouped_imputed_less_cols_df%>% slice_sample(n=1000))
rf_mice_grouped_less_cols_model_result <- test_func(rf_mice_grouped_less_cols_model,mf_test_df)
```

### NEED TO REMOVETHE SAMPLE SLICE
```{r}
#function that takes in list of dataframes and trains a model on each one.stores the y_test vector
loop_func_random_forest <- function(list_of_dfs,test_df){
    # List to store result vectors
    results_list <- list()
        # set up the loop of the list of dataframes
        for (i in c(1:length(list_of_dfs))) {
        # train the model
        temp_model <- random_forest_train_func(data.frame(list_of_dfs[[i]]) %>%  slice_sample(n=100))
        # make prediction on test set and store y_pred vector
        results_list[i] <- test_func(temp_model,mf_test_df)[1]
        }
    # put together all result vectors in dataframe
    res_df <- data.frame(cbind(results_list[[1]],results_list[[2]],results_list[[3]],
            results_list[[4]],results_list[[5]]))
    
    # calculate the average of all predictions across the 5 models.
    res_df <- res_df %>% mutate(mean_pred=rowMeans(across()))
    
    # calculate rmse on the averaged vector
    mean_sqrt <- sqrt(mean((test_df$GammaGTValue- res_df[,'mean_pred'])^2))
    
    return(list(res_df,mean_sqrt))
    
}
```

loop over all cols datasets
```{r}
#loop all cols df
loop_func_random_forest(imputed_df_list,mf_test_df)
```

loop over all cols datasets
```{r}
#loop less cols df
loop_func_random_forest(imputed_df_less_cols_list,mf_test_df)
```




# XGB using Caret
```{r}
# XGboost model

xgb_train_func <- function(train_data,test_data){
    # train control for CV
    # ctrl <-  trainControl(method = 'cv', number = 5, verboseIter = T)
    # 
    # xgb_grid <- expand.grid(nrounds = c(50,100,200),      #B - number of trees
    #                         max_depth = c(8:12),      #d - interaction depth
    #                         eta = c(0.1,0.01),       #lambda - learning rate
    #                         gamma = 0.001,            #mindev
    #                         colsample_bytree = 0.8,     #proportion random features per tree
    #                         min_child_weight = 1,     #also controls tree depth
    #                         subsample = 0.8             #bootstrap proportion
    # )
    
        # quick testtrain control for CV
    ctrl <-  trainControl(method = 'cv', number = 2, verboseIter = T)
    
    xgb_grid <- expand.grid(nrounds = c(50),      #B - number of trees
                            max_depth = c(8,10),      #d - interaction depth
                            eta = c(0.1,0.01),       #lambda - learning rate
                            gamma = 0.001,            #mindev
                            colsample_bytree = 0.8,     #proportion random features per tree
                            min_child_weight = 1,     #also controls tree depth
                            subsample = 0.8             #bootstrap proportion
    )
    
    xgb_final <- train(GammaGTValue ~ ., data = train_data,
                     method = 'xgbTree',
                     trControl = ctrl,
                     verbose = F,
                     tuneGrid = xgb_grid)
    # 
    # #Predictions
    # xgb_pred <- predict(xgb_final, test_data)
    # xgb_rmse <- sqrt(mean((test_data$GammaGTValue - xgb_pred)^2)) 
    
    return(xgb_final)
}

```


wide dataset
```{r}
xgb_wide_model<- xgb_train_func(wide_train_df %>% slice_sample(n=1000))
xgb_wide_model_result <- test_func(xgb_wide_model,mf_test_df)
```

miss forest dataset all
```{r}
xgb_mf_all_model <- xgb_train_func(mf_train_df  %>% slice_sample(n=1000))
xgb_mf_all_model_result <- test_func(xgb_mf_all_model,mf_test_df)
```

miss forest with fewer columns
```{r}
xgb_mf_less_cols_model <- xgb_train_func(mf_train_less_cols_df %>% slice_sample(n=1000))
xgb_mf_less_cols_model_result <- test_func(xgb_mf_less_cols_model,mf_test_df)
```

MICE imputed all columns agg dataset
```{r}
xgb_mice_grouped_all_model <- xgb_train_func(grouped_imputed_df%>% slice_sample(n=1000))
xgb_mice_grouped_all_model_result <- test_func(xgb_mice_grouped_all_model,mf_test_df)
```

MICE imputed fewer columns agg dataset
```{r}
xgb_mice_grouped_less_cols_model <- xgb_train_func(grouped_imputed_less_cols_df%>% slice_sample(n=1000))
xgb_mice_grouped_less_cols_model_result <- test_func(xgb_mice_grouped_less_cols_model,mf_test_df)
```


### NEED TO REMOVETHE SAMPLE SLICE
```{r}
#function that takes in list of dataframes and trains a model on each one.stores the y_test vector
loop_func_xgb <- function(list_of_dfs,test_df){
    # List to store result vectors
    results_list <- list()
        # set up the loop of the list of dataframes
        for (i in c(1:length(list_of_dfs))) {
        # train the model
        temp_model <- xgb_train_func(data.frame(list_of_dfs[[i]]) %>%  slice_sample(n=100))
        # make prediction on test set and store y_pred vector
        results_list[i] <- test_func(temp_model,mf_test_df)[1]
        }
    # put together all result vectors in dataframe
    res_df <- data.frame(cbind(results_list[[1]],results_list[[2]],results_list[[3]],
            results_list[[4]],results_list[[5]]))
    
    # calculate the average of all predictions across the 5 models.
    res_df <- res_df %>% mutate(mean_pred=rowMeans(across()))
    
    # calculate rmse on the averaged vector
    mean_sqrt <- sqrt(mean((test_df$GammaGTValue- res_df[,'mean_pred'])^2))
    
    return(list(res_df,mean_sqrt))
    
}
```

loop over all cols datasets
```{r}
loop_func_xgb(imputed_df_list,mf_test_df)
```
loop over less cols datasets
```{r}
loop_func_xgb(imputed_df_less_cols_list,mf_test_df)
```

```{r}
save.image(file="C:/Users/X464371/OneDrive - University of Cape Town/Yevashan Perumal MSc - OneDrive/Data/modelling_image.Rdata")
```