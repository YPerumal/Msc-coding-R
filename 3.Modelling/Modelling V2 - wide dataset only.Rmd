# Import Libraries
```{r message=FALSE, warning=FALSE}
library(xgboost)
library(tidyverse)
library(caret)
library(randomForest)
library(ggplot2)
library(rpart)
library(tree)
library(beepr)
#Imputation
library(mice)
library(Amelia)
library(Rcpp)
library('fastDummies')
library(missForest)
```

# Import datasets 
```{r}
# From the Impuation and Splits script
# load("C:/Users/X464371/OneDrive - University of Cape Town/Yevashan Perumal MSc - OneDrive/Data/imput_splits_image.Rdata")

# Current saved workspace
load("C:/Users/X464371/OneDrive - University of Cape Town/Yevashan Perumal MSc - OneDrive/Data/post_imputation_output_data.Rdata")
```

# Basic Baseline
```{r}
std.dev <- sqrt(var(wide_train_df$GammaGTValue))
```

# Random Forest Using Caret
```{r}
# Random forest

random_forest_func <- function(train_data,test_data){
    # Set train control for cross-val
    rf_ctrl <- trainControl(method = 'cv',
                            number = 5,
                            verboseIter = F)
    
    # Set grid search for hyperparaneter search
    rf_grid <- expand.grid(mtry = 10:20,
                           splitrule = 'variance', #Have to specify. This is RSS for reg.
                           min.node.size = c(5:10)) #Default for regression is 5. Controls tree size.
    
    # set.seed(42)
    rf_gridsearch <- train(GammaGTValue ~ ., 
                           data = train_data,
                           method = 'ranger',
                           num.trees = 100,
                           verbose = T,
                           trControl = rf_ctrl
                           ,tuneGrid = rf_grid #Here is the grid
                           ) 
    
    # rf_gridsearch$bestTune
    # mtry <- rf_gridsearch$bestTune[[1]]
    # splitrule <- rf_gridsearch$bestTune[[2]]
    # min.node.size <- rf_gridsearch$bestTune[[3]]
    
    # train a final model to get variable importance plot
    # rf_final <- randomForest(GammaGTValue ~ ., data = train_df,
    #                            ntree = 10,
    #                            importance = TRUE,
    #                             mtry=mtry,
    #                             splitrule=splitrule,
    #                             min.node.size=min.node.size)
    
    #Predictions
    rf_grid_pred <- predict(rf_gridsearch, newdata = test_data)
    rf_rmse <-  sqrt(mean((test_data$GammaGTValue- rf_grid_pred)^2))
    
    return(rf_rmse)
}
```

wide dataset
```{r}
rf_wide_test_rmse<- random_forest_func(wide_train_df %>% slice_sample(n=1000),mf_test_df)
rf_wide_test_rmse
```

miss forest dataset all
```{r}
rf_mf_all_test_rmse <- random_forest_func(mf_train_df  %>% slice_sample(n=1000),mf_test_df)
rf_mf_all_test_rmse
```
miss forest with fewer columns
```{r}
rf_mf_less_cols_test_rmse <- random_forest_func(mf_train_less_cols_df %>% slice_sample(n=1000),mf_test_df)
rf_mf_less_cols_test_rmse
```
MICE imputed all columns
```{r}
mice_grouped_test_rmse <- random_forest_func(grouped_imputed_df%>% slice_sample(n=1000),mf_test_df)
mice_grouped_test_rmse
```

MICE imputed fewer columns
```{r}
mice_grouped_less_cols_test_rmse <- random_forest_func(grouped_imputed_less_cols_df%>% slice_sample(n=1000),mf_test_df)
mice_grouped_less_cols_test_rmse
```


loop over multiple mice imputed datasets with all columns
```{r}
results_list_all_cols <- list( )
for (i in c(1:5)) {
    results_list_all_cols[i] <- random_forest_func(data.frame(imputed_df_list[[i]]) %>%  slice_sample(n=1000),mf_test_df)
}
```

loop over multiple mice imputed datasets with less columns
```{r}
results_list_less_cols <- list( )
for (i in c(1:5)) {
    results_list_all_cols[i] <- random_forest_func(data.frame(imputed_df_list[[i]]) %>%  slice_sample(n=1000),mf_test_df)
}
```



# XGB using Caret
```{r}
# XGboost model

xgb_func <- function(train_data,test_data){
    # train control for CV
    # ctrl <-  trainControl(method = 'cv', number = 5, verboseIter = T)
    # 
    # xgb_grid <- expand.grid(nrounds = c(50,100,200),      #B - number of trees
    #                         max_depth = c(8:12),      #d - interaction depth
    #                         eta = c(0.1,0.01),       #lambda - learning rate
    #                         gamma = 0.001,            #mindev
    #                         colsample_bytree = 0.8,     #proportion random features per tree
    #                         min_child_weight = 1,     #also controls tree depth
    #                         subsample = 0.8             #bootstrap proportion
    # )
    
        # quick testtrain control for CV
    ctrl <-  trainControl(method = 'cv', number = 2, verboseIter = T)
    
    xgb_grid <- expand.grid(nrounds = c(50),      #B - number of trees
                            max_depth = c(8,10),      #d - interaction depth
                            eta = c(0.1,0.01),       #lambda - learning rate
                            gamma = 0.001,            #mindev
                            colsample_bytree = 0.8,     #proportion random features per tree
                            min_child_weight = 1,     #also controls tree depth
                            subsample = 0.8             #bootstrap proportion
    )
    
    xgb_final <- train(GammaGTValue ~ ., data = train_data,
                     method = 'xgbTree',
                     trControl = ctrl,
                     verbose = F,
                     tuneGrid = xgb_grid)
    
    #Predictions
    xgb_pred <- predict(xgb_final, test_data)
    xgb_rmse <- sqrt(mean((test_data$GammaGTValue - xgb_pred)^2)) 
    
    return(xgb_rmse)
}


```

```{r}
xgb_wide_test_rmse<- xgb_func(wide_train_df,mf_test_df)
xgb_wide_test_rmse
```


```{r}
xgb_mf_all_test_rmse <- xgb_func(mf_train_df,mf_test_df)
xgb_mf_all_test_rmse
```

```{r}
xgb_mf_less_cols_test_rmse <- xgb_func(mf_train_less_cols_df,mf_test_df)
xgb_mf_less_cols_test_rmse
```
```{r}
save.image(file="C:/Users/X464371/OneDrive - University of Cape Town/Yevashan Perumal MSc - OneDrive/Data/modelling_image.Rdata")
```





# Extra Code

## Decision Tree

-- Gonna can this for the moment

Used Rpart and Caret
```{r}
# Decision Trees
# Set train control for cross-val
# dt_ctrl <- trainControl(method = 'cv', number = 5, verboseIter = F)
# 
# #rpart
# dt_grid <- expand.grid(cp = c(0.0001,0.001,0.01,0.05,0.1)) #Default for regression is 5. Controls tree size.
# 
# # # rpart2
# # dt_grid <- expand.grid(maxdepth = c(5,10,20)
# #                        ) #Default for regression is 5. Controls tree size.
# 
# set.seed(42)
# dt_gridsearch <- train(GammaGTValue ~ .,
#                        data = train_df,
#                        method = 'rpart',
#                        trControl = dt_ctrl
#                        ,tuneGrid = dt_grid #Here is the grid
#                        )
# 
# # dt_gridsearch$bestTune
# # cp <- dt_gridsearch$bestTune[[1]]
# 
# # train a final model to get variable importance plot
# # library(rpart)
# # dt_final <-  rpart(GammaGTValue ~ ., data = train_df,cp=cp)
# 
# #Predictions
# dt_grid_pred <- predict(dt_gridsearch, newdata = test_df)
# dt_rmse_long <-  sqrt(mean((test_df$GammaGTValue- dt_grid_pred)^2))
```


Using the Trees Library
```{r}
# Decision Trees
# set.seed(42)
# 
# tree_model <- tree(GammaGTValue ~ ., data=train_df)
# summary(tree_model)
# 
# plot(tree_model)
# text(tree_model, cex=0.8)
# 
# # cross validation
# cv_tree <- cv.tree(tree_model)
# plot(cv_tree$size, cv_tree$dev, type = 'b')
# 
# ## pruning
# prune_tree <- prune.tree(tree_model, best = 2)
# plot(prune_tree)
# text(prune_tree, pretty=0, cex=0.8)
# 
# tree_yhat<- predict(tree_model, newdata=x_test)
# tree_res <- data.frame(tree_yhat,y_test)
# plot(tree_res$tree_yhat, tree_res$tree_yhat)
# 
# tree_rmse <- sqrt(mean((tree_yhat-y_test$GammaGTValue)^2))# MSE
```

