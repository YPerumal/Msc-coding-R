---
title: "R Notebook"
output: html_notebook
---

# Import Libraries
```{r message=FALSE, warning=FALSE}
library(xgboost)
library(tidyverse)
library(caret)
library(randomForest)
library(ggplot2)
library(caret)
library(gbm)
library(tree)
library(xgboost)
```

# Import Data
```{r}
# df_wide <- read.csv('/Users/yevashanperumal/Library/CloudStorage/OneDrive-UniversityofCapeTown/Yevashan Perumal MSc - OneDrive/Data/wide_data.csv')

df_long <- read.csv('/Users/yevashanperumal/Library/CloudStorage/OneDrive-UniversityofCapeTown/Yevashan Perumal MSc - OneDrive/Data/long_data.csv')
```

# Train-Test Split for df_long
```{r}
# Create the training and test datasets
set.seed(42)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(df_long$GammaGTValue, p=0.8, list=FALSE)

# Step 2: Create the training  dataset
train_df <- df_long[trainRowNumbers,]
x_train <- train_df %>% select(-GammaGTValue)
y_train <- train_df %>% select(GammaGTValue)

# Step 3: Create the test dataset
test_df <- df_long[-trainRowNumbers,]
x_test <- test_df %>% select(-GammaGTValue)
y_test <- test_df %>% select(GammaGTValue)
```

# Decision Tree
```{r}
# Random forest
# Set train control for cross-val
dt_ctrl <- trainControl(method = 'cv', number = 5, verboseIter = F)

# Set grid search for hyperparaneter search
# dt_grid <- expand.grid(mtry = 3:5,
#                        splitrule = 'variance', #Have to specify. This is RSS for reg.
#                        min.node.size = c(1,5)) #Default for regression is 5. Controls tree size.

# set.seed(42)
dt_gridsearch <- train(GammaGTValue ~ ., 
                       data = train_df,
                       method = 'rpart',
                       trControl = dt_ctrl
                       # ,tuneGrid = dt_grid #Here is the grid
                       ) 
#
# dt_gridsearch$bestTune
# mtry <- dt_gridsearch$bestTune[[1]]
# splitrule <- dt_gridsearch$bestTune[[2]]
# min.node.size <- dt_gridsearch$bestTune[[3]]

# train a final model to get variable importance plot
library(tree)
dt_final <-  tree(GammaGTValue ~ ., data = train_df)
summary(dt_final)
#Predictions
dt_grid_pred <- predict(dt_final, newdata = test_df)
dt_rmse <-  sqrt(mean((test_df$GammaGTValue- dt_grid_pred)^2))
```



# Random Forest Using Caret
```{r}
# Random forest
# Set train control for cross-val
rf_ctrl <- trainControl(method = 'cv', number = 5, verboseIter = F)

# Set grid search for hyperparaneter search
rf_grid <- expand.grid(mtry = 3:5,
                       splitrule = 'variance', #Have to specify. This is RSS for reg.
                       min.node.size = c(1,5)) #Default for regression is 5. Controls tree size.

# set.seed(42)
rf_gridsearch <- train(GammaGTValue ~ ., 
                       data = train_df,
                       method = 'ranger',
                       num.trees = 10,
                       verbose = T,
                       trControl = rf_ctrl
                       ,tuneGrid = rf_grid #Here is the grid
                       ) 

rf_gridsearch$bestTune
mtry <- rf_gridsearch$bestTune[[1]]
splitrule <- rf_gridsearch$bestTune[[2]]
min.node.size <- rf_gridsearch$bestTune[[3]]

# train a final model to get variable importance plot
rf_final <- randomForest(GammaGTValue ~ ., data = train_df,
                           ntree = 10,
                           importance = TRUE,
                            mtry=mtry,
                            splitrule=splitrule,
                            min.node.size=min.node.size)

#Predictions
rf_grid_pred <- predict(rf_final, newdata = test_df)
rf_rmse <-  sqrt(mean((test_df$GammaGTValue- rf_grid_pred)^2))
```

XGB using Caret
```{r}
# XGboost model
xgb_grid <- expand.grid(nrounds = c(10,25),  #B - number of trees
                        max_depth = c(1,5),      #d - interaction depth
                        eta = c(0.1,0.01),       #lambda - learning rate
                        gamma = 0.001,            #mindev
                        colsample_bytree = 1,     #proportion random features per tree
                        min_child_weight = 1,     #also controls tree depth
                        subsample = 1             #bootstrap proportion
)
ctrl <-  trainControl(method = 'cv', number = 5, verboseIter = T)

xgb_final <- train(GammaGTValue ~ ., data = train_df,
                 method = 'xgbTree',
                 trControl = ctrl,
                 verbose = F,
                 tuneGrid = xgb_grid)

#Predictions
xgb_pred <- predict(xgb_final, test_df)
xgb_rmse <- sqrt(mean((test_df$GammaGTValue - xgb_pred)^2))

```

